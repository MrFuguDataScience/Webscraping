{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python Webscraping Indeed (2021) Revisit\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "**Purpose & Outcome:**\n",
    "Webscrape Indeed: take position, job title,location,date of posting, string of qualifications\n",
    "\n",
    "Use a list of words related to your skills or skills you are interested in and extract from job post qualifications section.\n",
    "\n",
    "Date time formating\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests         # grab web-page\n",
    "import pickle           # save file\n",
    "from bs4 import BeautifulSoup as bsopa  # parse web-page\n",
    "import datetime         # format date/time\n",
    "from collections import defaultdict\n",
    "import re               # regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ex.) https://www.indeed.com/jobs?q=data+scientist&l=california&start=10\n",
    "\n",
    "range(0,150,10): each page will have \"start=0,start=10,start=20 etc\" which deals with\n",
    "going through each page, but not exactly 10 entries/pg. \n",
    "\n",
    "string formatting is used to denote what job and location we want: you can use a string\n",
    "separated by space and it will be interpreted by the website. \n",
    "\n",
    "sou=bsopa(y.text,'lxml') is taking our get request and converting to text in the format\n",
    "of 'lxml' but we can replace with 'html.parser' as well.\n",
    "\n",
    "Each job post can be parsed by the 'div',{\"class\":\"jobsearch-SerpJobCard\"} or \n",
    "'div', {'class': 'row'} depending on how you want to search\n",
    "\n",
    "After that we get each piece of information we want to obtain: job title, location etc.\n",
    "\n",
    "the only difficult and frustrating part is getting all the raw text for each posting\n",
    "relating to the qulifications. We have to open a link, iterate through it and then extract\n",
    "all the information with a try/except block and then further process.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg=[]\n",
    "for j in range(0,150,10):\n",
    "    position,location='data scientist','california'\n",
    "    y=requests.get('https://www.indeed.com/jobs?q={}&l={}&sort=date='.format(position,location)+str(j))\n",
    "\n",
    "    # y=requests.get('https://www.indeed.com/jobs?q=data+scientist&l=california&sort=date='+str(i))\n",
    "    sou=bsopa(y.text,'lxml')\n",
    "\n",
    "#     for ii in sou.find_all('div', {'class': 'row'}):\n",
    "    for ii in sou.find_all('div',{\"class\":\"jobsearch-SerpJobCard\"}):\n",
    "#         print(ii)\n",
    "\n",
    "        job_title = ii.find('a', {'data-tn-element': 'jobTitle'})['title']\n",
    "        company_name = ii.find('span', {'class': 'company'}).text.strip()    \n",
    "        location=ii.find('span',{\"class\":\"location\"})\n",
    "        post_date = ii.find('span', attrs={'class': 'date'})\n",
    "        summary=ii.find('div',attrs={'class':'summary'})\n",
    "\n",
    "        if location:\n",
    "            location=location.text.strip()\n",
    "        else:\n",
    "            location=ii.find('div',{\"class\":\"location\"})\n",
    "            location=location.text.strip()\n",
    "\n",
    "        k=ii.find('h2', {'class':\"title\"})\n",
    "        p=k.find(href=True)\n",
    "        v=p['href']\n",
    "        f_=str(v).replace('&amp;','&') # links to iterate for qualification text\n",
    "        \n",
    "        \n",
    "        datum = {'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'location': location,\n",
    "                    'summary':summary.text.strip(),\n",
    "                    'post_Date':post_date.text,\n",
    "                    'Qualification_link': f_}\n",
    "\n",
    "        print(datum)\n",
    "        gg.append([location,job_title,company_name,post_date.text,summary.text.strip()\n",
    "#                   ,f_]) \n",
    "#         gg.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Senior Data Scientist - Ads Optimization\n",
      "Data Scientist (Ondot)\n",
      "Data Scientist I\n",
      "Data Scientist, Monetization\n",
      "Data Scientist - Entry Level\n",
      "Professional Data Scientist\n",
      "Data Scientist\n",
      "Senior Data Scientist - Flexible Location, CA\n",
      "Data Scientist ( FULL TIME)\n",
      "Senior Data Scientist - Ads Optimization\n",
      "Data Scientist (Ondot)\n",
      "Data Scientist I\n",
      "Data Scientist, Monetization\n",
      "Data Scientist - Entry Level\n",
      "Professional Data Scientist\n",
      "Data Scientist\n",
      "Senior Data Scientist - Flexible Location, CA\n",
      "Data Scientist ( FULL TIME)\n"
     ]
    }
   ],
   "source": [
    "gg=[]\n",
    "for j in range(0,15,10):\n",
    "    position,location='data scientist','california'\n",
    "    y=requests.get('https://www.indeed.com/jobs?q={}&l={}&sort=date='.format(position,location)+str(j))\n",
    "#     print(y)\n",
    "    # y=requests.get('https://www.indeed.com/jobs?q=data+scientist&l=california&sort=date='+str(i))\n",
    "    sou=bsopa(y.text,'lxml')\n",
    "#     print(sou)\n",
    "    for ii in sou.find_all('div',{\"class\":\"job_seen_beacon\"}):\n",
    "        j=ii.find('tbody')\n",
    "        a= j.find('tr')\n",
    "        for n in a.find_all('h2',{'class':'jobTitle jobTitle-color-purple jobTitle-newJob'}):\n",
    "            print(n.find_all('span')[1].get_text())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.25"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "125*.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datum' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7c5ad11b677b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datum' is not defined"
     ]
    }
   ],
   "source": [
    "datum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
