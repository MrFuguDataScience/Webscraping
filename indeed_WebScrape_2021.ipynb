{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python Webscraping Indeed (2021) Revisit\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "**Purpose & Outcome:**\n",
    "Webscrape Indeed: take position, job title,location,date of posting, string of qualifications\n",
    "\n",
    "Use a list of words related to your skills or skills you are interested in and extract from job post qualifications section.\n",
    "\n",
    "Date time formating\n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests         # grab web-page\n",
    "import pickle           # save file\n",
    "from bs4 import BeautifulSoup as bsopa  # parse web-page\n",
    "import datetime         # format date/time\n",
    "from collections import defaultdict\n",
    "import re               # regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ex.) https://www.indeed.com/jobs?q=data+scientist&l=california&start=10\n",
    "\n",
    "range(0,150,10): each page will have \"start=0,start=10,start=20 etc\" which deals with\n",
    "going through each page, but not exactly 10 entries/pg. \n",
    "\n",
    "string formatting is used to denote what job and location we want: you can use a string\n",
    "separated by space and it will be interpreted by the website. \n",
    "\n",
    "sou=bsopa(y.text,'lxml') is taking our get request and converting to text in the format\n",
    "of 'lxml' but we can replace with 'html.parser' as well.\n",
    "\n",
    "Each job post can be parsed by the 'div',{\"class\":\"jobsearch-SerpJobCard\"} or \n",
    "'div', {'class': 'row'} depending on how you want to search\n",
    "\n",
    "After that we get each piece of information we want to obtain: job title, location etc.\n",
    "\n",
    "the only difficult and frustrating part is getting all the raw text for each posting\n",
    "relating to the qulifications. We have to open a link, iterate through it and then extract\n",
    "all the information with a try/except block and then further process.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg=[]\n",
    "for j in range(0,150,10):\n",
    "    position,location='data scientist','california'\n",
    "    y=requests.get('https://www.indeed.com/jobs?q={}&l={}&sort=date='.format(position,location)+str(j))\n",
    "\n",
    "    # y=requests.get('https://www.indeed.com/jobs?q=data+scientist&l=california&sort=date='+str(i))\n",
    "    sou=bsopa(y.text,'lxml')\n",
    "\n",
    "#     for ii in sou.find_all('div', {'class': 'row'}):\n",
    "    for ii in sou.find_all('div',{\"class\":\"jobsearch-SerpJobCard\"}):\n",
    "#         print(ii)\n",
    "\n",
    "        job_title = ii.find('a', {'data-tn-element': 'jobTitle'})['title']\n",
    "        company_name = ii.find('span', {'class': 'company'}).text.strip()    \n",
    "        location=ii.find('span',{\"class\":\"location\"})\n",
    "        post_date = ii.find('span', attrs={'class': 'date'})\n",
    "        summary=ii.find('div',attrs={'class':'summary'})\n",
    "\n",
    "        if location:\n",
    "            location=location.text.strip()\n",
    "        else:\n",
    "            location=ii.find('div',{\"class\":\"location\"})\n",
    "            location=location.text.strip()\n",
    "\n",
    "        k=ii.find('h2', {'class':\"title\"})\n",
    "        p=k.find(href=True)\n",
    "        v=p['href']\n",
    "        f_=str(v).replace('&amp;','&') # links to iterate for qualification text\n",
    "        \n",
    "        \n",
    "        datum = {'job_title': job_title,\n",
    "                    'company_name': company_name,\n",
    "                    'location': location,\n",
    "                    'summary':summary.text.strip(),\n",
    "                    'post_Date':post_date.text,\n",
    "                    'Qualification_link': f_}\n",
    "\n",
    "        print(datum)\n",
    "        gg.append([location,job_title,company_name,post_date.text,summary.text.strip()\n",
    "#                   ,f_]) \n",
    "#         gg.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunnyvale, CA 94086 (Washington area)+1 location•Temporarily Remote\n",
      "San Jose, CA\n",
      "Cupertino, CA\n",
      "Rancho Cucamonga, CA 91730\n",
      "Mountain View, CA 94041 (Old Mountain View area)\n",
      "Livermore, CA 94550\n",
      "El Segundo, CA 90245\n",
      "Santa Clara, CA•Remote\n",
      "Los Angeles, CA\n",
      "San Francisco, CA 94105 (Financial District area)\n",
      "Sunnyvale, CA 94086 (Washington area)+1 location•Temporarily Remote\n",
      "Cupertino, CA\n",
      "San Jose, CA\n",
      "Rancho Cucamonga, CA 91730\n",
      "Mountain View, CA 94041 (Old Mountain View area)\n",
      "Livermore, CA 94550\n",
      "El Segundo, CA 90245\n",
      "Santa Clara, CA•Remote\n",
      "Los Angeles, CA\n",
      "San Francisco, CA 94105 (Financial District area)\n"
     ]
    }
   ],
   "source": [
    "gg=[]\n",
    "for j in range(0,15,10): # calling 15 entries\n",
    "    \n",
    "    position,location='data scientist','california'\n",
    "    y=requests.get('https://www.indeed.com/jobs?q={}&l={}&sort=date='.format(position,location)+str(j))\n",
    "#     print(y)\n",
    "    # y=requests.get('https://www.indeed.com/jobs?q=data+scientist&l=california&sort=date='+str(i))\n",
    "    sou=bsopa(y.text,'lxml')\n",
    "    \n",
    "#     print(sou) use this if you want to check if working properly, response code 200\n",
    "    for ii in sou.find_all('div',{\"class\":\"job_seen_beacon\"}):\n",
    "        j=ii.find('tbody') # calling the table body to go inside of\n",
    "        a= j.find('tr') # going inside the table\n",
    "            \n",
    "        for n in a.find_all('h2',{'class':'jobTitle jobTitle-color-purple jobTitle-newJob'}):\n",
    "            job_title=n.find_all('span')[1].get_text() # if you don't use the 1, you get the 'new' posting text\n",
    "            \n",
    "            # Company Name is in new nesting:\n",
    "#             print(a.find_all('span',{'class':'companyName'}))\n",
    "            other=a.find('div',{'class':'heading6 company_location tapItem-gutter'})\n",
    "            pr_=other.find('span')\n",
    "            company_=(pr_.get_text())\n",
    "#             print(company_) \n",
    "            \n",
    "        # Location:\n",
    "            locaiton=(other.find('div',{'class':'companyLocation'}).get_text())\n",
    "            \n",
    "        # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
