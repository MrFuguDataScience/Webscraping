{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d0f06c",
   "metadata": {},
   "source": [
    "# `Cloud Flare Thoughts`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860c2d9",
   "metadata": {},
   "source": [
    "# `Background:`\n",
    "\n",
    "Have you ever tried to webscrape and get bloced by your IP address, have constant CAPTCHA's appeared in a loop that won't let you leave? Stumble onto data that is hidden that you cannot retreive? It can be annoying when you are learning (webscraping) or building a dataset. Sites such as `Cloudflare`, `PerimeterX`, `Akamai` are capable of even noticing when you are using a headless browser. We should consider understanding how this works and what these servers are using to evaluate our request(s).\n",
    "\n",
    "\n",
    "\n",
    "# `What we will cover today:`\n",
    "\n",
    "Today, I would like to cover a few ideas and tips to help you from detection. Really we need to rethink and leverage our skills to mimic a real user.\n",
    "\n",
    "\n",
    "\n",
    "# <font color=red> DISCLAIMER</FONT>:\n",
    "\n",
    "**DO NOT ACT LIKE A PIRATE PILAGING AND PLUNDERING. ACT RESPONSIBLE & ETHICALLLY WHILE COLLECTING DATA WHEN WEBSCRAPING...**\n",
    "\n",
    "`----------------------------------------------------------------------`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba80911",
   "metadata": {},
   "source": [
    "# Cloudflare\n",
    "\n",
    "*Handling* `Cloudflare` *Protection*:\n",
    "\n",
    "`Cloudflare` uses various techniques to detect and block bots, such as JavaScript challenges and IP rate limiting.\n",
    "\n",
    "*Basic understaning* of how `Cloudflare` operates: \n",
    "\n",
    "`--------------------------------------`\n",
    "\n",
    "They are a cyber security and content delivery network. Because of this you are at a disposition as a webscraper because you are not spared from their wrath and considereda security issue. Using `Selenium WebDriver` is a warning beacon making your presence known. `CloudFlare` uses *active* and *passive* detecting measures to counter any perceived threat.\n",
    "\n",
    "*They utilize*:\n",
    "\n",
    "+ *`Active` monitoring:* CAPTCHA, event tracking, (canvas) fingerprinting to name a few\n",
    "+ *`Passive` monioring:* HTTP headers, TLS finger printing, checking your IP adress reputation, among many others\n",
    "+ May imploy behavior techniques such as *mouse movements*\n",
    "+ *JavaScript Challenges*\n",
    "\n",
    "\n",
    "I want to mention a few important tasks that may occur when using this platform when you are using `automation tasks/testing software`\n",
    "\n",
    "For example when you are trying to webscrape there are signatures that are looked for to determine your behavior and finger print.\n",
    "\n",
    "+ When using `Selenium` you have a `WebDriver` that is used to connect a browser such as Chrome, Firefox, Mozilla, etc to that server through requests.\n",
    "    + By default `Selenium` will send information to the website letting it know you are using active test-autiomation software. This trigger can directly impact your interaction with a server. Yes, you can mask this by turning off the ping but, it is not fool proof.\n",
    "\n",
    "*Other behaviors can trigger this as well:*\n",
    "    \n",
    "+ When you try to connect to a server; information about you is given to them such as:\n",
    "    + what version of browser, what type of browser you are on, screen resolution, mouse movements, timing of your responses like clicks and moving to new pages quickly, geo-location, IP address.\n",
    "    \n",
    "*Think of it like this*: we are creating an active digital finger print. This finger print consists of various markers that make the site able to detect who we are. Because of this you have to consider that sites can evaluate your IP Address as credible or potentially a bot and they hold the keys to let you pass.\n",
    "\n",
    "+ If you want to obscure your presence and fly under the radar it is imperative to include various steps in your process to roam around without raising suspicion. \n",
    "\n",
    "**Considerations:** to help in your epic learning experience as a data extracting boss (ethically of course...)\n",
    "\n",
    "`-------------------------------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038ad9a",
   "metadata": {},
   "source": [
    "# 1.) `Headers:`\n",
    "\n",
    "Are one of the easiest ways for an anti-bot server to detect you. This is because common HTTP libraries such as (*Python Request, Scrapy or NodeJS Axiom*) by default *`ignore adding browser headers or the headers that are included indicate its from a library`*\n",
    "\n",
    "+ `Header order is very important and overlooked`, for instance when using HTTP client(s)/libraries we have issues where the order can be random or appear as it came from a python library which is bad. `Python Request` is an example of this.\n",
    "    + `HTTX` is a better option because it does respect the ordering and it also gives you `HTTP/2`\n",
    "\n",
    "+ Some websites will benefit from you changing what headers you're using because of how they are retreived from a server.\n",
    "\n",
    "`------------------------------------------------------------`\n",
    "\n",
    "**Solution:** consider turning off notifcations from Selenium showing that you are using automation software tools. With regard to Python we need to consider options for the header such as `headless browser` but this poses other issues I will mention.\n",
    "\n",
    "+ Turn off blink feature: `options.add_argument('--disable-blink-features=AutomationControlled')`\n",
    "\n",
    "+ Using a `headless browser` does have inherent problems as well for detection to some servers. A way to deal with it would be creating `headeless browser`, adding `user-agent` and the adjusting a `window size` and dealing with your `headers`. This is a good start and work from there adding to the code to help you seem more legit.\n",
    "\n",
    "**ex.)** \n",
    "\n",
    "`from selenium import webdriver`\n",
    "\n",
    "`from selenium_stealth import stealth`\n",
    "\n",
    "`from selenium.webdriver.chrome.service import Service`\n",
    "\n",
    "`from selenium.webdriver.chrome.options import Options`\n",
    "\n",
    "`from webdriver_manager.chrome import ChromeDriverManager`\n",
    "\n",
    "\n",
    "`options = Options()`\n",
    "\n",
    "`options.add_argument(\"--headless\")  # Optional: Run in headless mode` \n",
    "\n",
    "**this may be setup different like: `options.headless = True` or `options.add_argument(\"--headless=new\")` depending what version of selenium you are using!!!!**\n",
    "\n",
    "`options.add_argument(\"--window-size=1920,1200\")  # Set the window size`\n",
    "\n",
    "`user_agents = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "    # Add more User-Agent strings here\n",
    "]`\n",
    "\n",
    "`options.add_argument(f'user-agent={random.choice(user_agents)}')`\n",
    "\n",
    "\n",
    "`driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)`\n",
    "\n",
    "`driver.get('https://www.example.com')`\n",
    "\n",
    "\n",
    "`---------------------------------------------------------`\n",
    "\n",
    "+ Use a header and user agent even when going headless by setting it explicitly yourself and consider setting a window as well.\n",
    "\n",
    "**`ALWAYS use real headers from a broswer and user-agent`**\n",
    "\n",
    "\n",
    "If you need to check if the user agent you are using for webscraping is correct run this test:\n",
    "\n",
    " \n",
    "# Test if your web scraper is sending correct header to HTTPBin \n",
    "\n",
    "*this is an example*\n",
    "\n",
    "`import requests `\n",
    " \n",
    "`headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\"}` \n",
    "\n",
    "`r = requests.get(\"https://httpbin.org/headers\", headers=headers)` \n",
    "\n",
    "`print(r.text)`\n",
    "\n",
    "\n",
    "`-------------------------------------------------------------`\n",
    "\n",
    "\n",
    "# `Examples how browsers can detect bot/automation tool (within the Header):`\n",
    "\n",
    "\n",
    "1.) **`Possible User-Agent Issues:`** bots/automation tools can add stings that defeat everything in the header\" \n",
    "\n",
    "ex.) `scraper,bot,crawler` this is a dead giveaway.\n",
    "\n",
    "`----------------`\n",
    "\n",
    "2.) Browsers such as `Google & Safari` add language fallbacks in tags called **`Accept Language:`**. \n",
    "\n",
    "+ Sometimes weird language may be in this section or completely inaccurate data. \n",
    "\n",
    "+ Notice that the `q` will always go in decreasing order such as\n",
    "\n",
    "ex.) `en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7` this will occur when navigator.language is [`en-US, zh-CN`]\n",
    "\n",
    "+ `Important note:` if you are using `Safari` or `Google Incognito` due to privacy for fingerprinting one language is specified.\n",
    "\n",
    "+ `Side note:` a browser may not give the language you are interested in for example if you are traveling or using a VPN that has a country with different language as your own. Consider this ----\n",
    "    + You may get an error of `406` (Unacceptable) which means your request could not be matched\n",
    "\n",
    "\n",
    "`----------------`\n",
    "\n",
    "\n",
    "3.) **`Accept Encoding:`** same issue as above, can be empty or wrong information. below is an example of what is more realistic. This is a compression algorithm during the negotiation of the handshake.\n",
    "\n",
    "ex.) \n",
    "\n",
    "`Accept-Encoding: gzip`\n",
    "\n",
    "`Accept-Encoding: gzip, compress, br`\n",
    "\n",
    "`Accept-Encoding: gzip, compress, br, zstd`\n",
    "\n",
    "`Accept-Encoding: br;q=1.0, gzip;q=0.8, *;q=0.1`\n",
    "\n",
    "\n",
    "`----------------`\n",
    "\n",
    "\n",
    "4.) **`Referer:`** bots tend to leave empty or add incorrect information. Here is an example of legit info. The purpose of this is where the request is coming from.\n",
    "\n",
    "ex.) `you can add an actual website here` but not always neccessary.\n",
    "\n",
    "`----------------`\n",
    "\n",
    "\n",
    "5.) **`Connection`** decides if conneciton stays open after the current transaction finishes. If the value `keep-alive` is used then the connection will continue to stay open after transaction. You also have `close` which describes either the client or server are requesting to close after transaction.\n",
    "\n",
    "+ `IMPORTANT NOTE:` `keep-alive` are prohibited in `HTTP/2 , HTTP/3`. *Chrome & Firefox HTTP/2* started to ignore this. `Safari HTTP/2` conforms to the spec requirements when using this (whatever those are) and will not load these responses.\n",
    "\n",
    "ex.) `Keep-Alive, Transfer-Encoding, TE, Connection, Trailer, Upgrade, Proxy-Authorization and Proxy-Authenticate` \n",
    "\n",
    "If these are used they must be used within the `Connection Header` letting the proxy know they need to be consumed and are not moved to further steps forward.\n",
    "\n",
    "\n",
    "`----------------`\n",
    "\n",
    "\n",
    "6.) **`Accept`** indicates what content type your client can understand. Depending on your request different types of data are returned. Such as if you ask for *images, scripts, CSS, etc*\n",
    "\n",
    "ex.) \n",
    "\n",
    "+ `text/html`\n",
    "\n",
    "+ `application/xhtml+xml`\n",
    "\n",
    "+ `application/xml;q=0.9`\n",
    "\n",
    "+ `image/webp`\n",
    "\n",
    "+ `image/apng`\n",
    "\n",
    "+ `*/*;q=0.8`\n",
    "\n",
    "`-------------------------------------------------------------`\n",
    "\n",
    "https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d36126",
   "metadata": {},
   "source": [
    "**`# Code Example:`**\n",
    "\n",
    "`from selenium import webdriver`\n",
    "\n",
    "`from selenium.webdriver.chrome.service import Service`\n",
    "\n",
    "`from selenium.webdriver.chrome.options import Options`\n",
    "\n",
    "`from webdriver_manager.chrome import ChromeDriverManager`\n",
    "\n",
    "`options = Options()`\n",
    "\n",
    "`driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)`\n",
    "\n",
    "*`# Customized header`*\n",
    "\n",
    "`driver.execute_cdp_cmd('Network.setUserAgentOverride', {'userAgent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)` \n",
    "\n",
    "`AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',`\n",
    "    `'acceptLanguage': 'en-US,en;q=0.9',`\n",
    "    `\n",
    "    'platform': 'Windows'`\n",
    "`})`\n",
    "\n",
    "`driver.get('https://www.example.com')`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45050b6e",
   "metadata": {},
   "source": [
    "`-------------------------------------------------------------`\n",
    "\n",
    "# 2.) **`IP Blocking/Tracking`**\n",
    "\n",
    "Your IP Address can be a red flag to these sites. Remember they have options such as throttling your requests, limiting data shown to you or even block you out right. If you are suspected to have a past of making a ton of requests, acting malicious or against terms of service this may persist into the future as an issue for you. I was blocked from Indeed webscraping twice with 3 different IP address in 2 different countries which persisted for around 9-12 months. Your geographic location can pose an issue as well because this may even restrict your access or the content you see. \n",
    "\n",
    "\n",
    "**`Solution:`** you will need to use **rotating proxy** services to obscure your IP address and utilize theirs. *VPN networks and TOR* can have issues that will not resolve themselves and cannot guarantee results.\n",
    "\n",
    "+ *`DO NOT use free proxies`*, they will get you banned for various reason. If you need to do inustrial scale scraping spend the money."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e78e0e",
   "metadata": {},
   "source": [
    "**`Ex.) Proxy:`**\n",
    "    \n",
    "    \n",
    "`from selenium import webdriver`\n",
    "\n",
    "`from selenium.webdriver.chrome.service import Service`\n",
    "\n",
    "`from selenium.webdriver.chrome.options import Options`\n",
    "\n",
    "`from webdriver_manager.chrome import ChromeDriverManager`\n",
    "\n",
    "`options = Options()`\n",
    "\n",
    "`options.add_argument('--proxy-server=http://your-proxy-address:port')`\n",
    "\n",
    "`driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)`\n",
    "\n",
    "`driver.get('https://www.example.com')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52186235",
   "metadata": {},
   "source": [
    "`-------------------------------------------------------------`\n",
    "\n",
    "# 3.) **`User Agent`**\n",
    "\n",
    "This is a string identifying the client making the request(s) and this information will identify if you are from a trusted or known browser.\n",
    "\n",
    "ex.) `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36`\n",
    "\n",
    "Typical webscraping bots/libraries make it very easy to tell where you are coming from and who you are  which reveal your intentions in the process.\n",
    "\n",
    "Think about the point of view from what a website. They see something making repeated calls in a short amount of time from the same user-agent. In my opinion you are playing with fire.\n",
    "\n",
    "**Important NOTE:** if you use `--headless` browser you will end up blindly showing what you are doing in the browser. You will need to add an additional flag to assist you\n",
    "\n",
    "**Ex.)** this is a snippet not full code...\n",
    "\n",
    "\n",
    "`#create a custom user agent`\n",
    "\n",
    "`Your_User_Agent = \"your info here\"`\n",
    "\n",
    "`options.add_argument(f'--user-agent={Your_User_Agent}')`\n",
    "\n",
    "`enable headless`\n",
    "\n",
    "`options.add_argument('--headless')`\n",
    "\n",
    "`-------------------------------------------`\n",
    "\n",
    "https://www.zenrows.com/blog/user-agent-web-scraping#best   (read this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16893f61",
   "metadata": {},
   "source": [
    "`-------------------------------------------------------------`\n",
    "\n",
    "# 4.) **`Java Script Challenge`** \n",
    "\n",
    "This is a sneaky little technique checking to see if you are legit or not. They will send you a piece of `Java Script` code and the issue is that most automation tools cannot answer the request because it doesn't render. This is how they catch you, think of Dave Chappelle in his famous line: \"Gottcha ...\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7af84e",
   "metadata": {},
   "source": [
    "`-------------------------------------------------------------`\n",
    "\n",
    "# 5.) **`Timing Requests`**\n",
    "\n",
    "Setting timeouts for loading pages, allowing for different times for various actions to occur and pagination will help you greatly in your journey. Make it less robotic and act dynamic and like a person.\n",
    "\n",
    "\n",
    "+ This can be very important when you need to have a page load before scraping because of dynamic content\n",
    "\n",
    "+ If you need to scroll a page with automation and use timing\n",
    "\n",
    "+ mouse movements\n",
    "\n",
    "+ click buttons\n",
    "\n",
    "+ randomize your movements scraping or creating a multi-threating process\n",
    "\n",
    "\n",
    "**`Two common ways`** add `time.sleep()` or \n",
    "\n",
    "`from selenium.webdriver.common.by import By`\n",
    "\n",
    "`from selenium.webdriver.support.ui import WebDriverWait`\n",
    "\n",
    "`from selenium.webdriver.support import expected_conditions as EC`\n",
    "\n",
    "`WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"Some_Element_YouWant\"))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119704e1",
   "metadata": {},
   "source": [
    "`-------------------------------------------------------------`\n",
    "\n",
    "# 6.) **`TLS Finger Printing:`**\n",
    "\n",
    "`TLS:` **T**ransport **L**ayer **S**ecurity, can be considered a handshake between you the client and the server. Information is exchanged to make sure you are legit, the communication is encrypted and each client will have their own signature (finger print). The information provided in your finger print dictates if it is a casual user or automated tools such as bots or scrapers. In general a request from a browser will be different from a programming language.\n",
    "\n",
    "+ This is very important because it can detect: `network spoofing, man in the middle attacks, potential espionage`\n",
    "\n",
    "+ When making a call (request) there is something called a `JA3 hash`, this is very important because it can be detected as you are the same person making repeated requests and end up giving you a lot of head aches very quickly. From my understanding starting in *Chrome 110* the TLS client side hello started to randomize this JA3 hash. \n",
    "\n",
    "+ One option to consider is `curl-cffi` because it is more aligned with HTTP protocol and TLS handshakes\n",
    "\n",
    "Unfortunately, for us this means that even though you were trying to be sleath it stumbles because your `requests does NOT change the JA3 hash`. Libaries such as `request` rely on the JA3 hash and this is a vulnerability exploited by the server side to hinder our work. \n",
    "\n",
    "`-------------------------------------------------------------`\n",
    "\n",
    "ex.) `import httpx` \n",
    "\n",
    "+ This can be used if you have issues with HTTP/1.1 and need HTTP/2 for instance\n",
    "\n",
    "ex.) [Java Selenium Finger printing](https://github.com/CheshireCaat/selenium-with-fingerprints)\n",
    "\n",
    "+ Change your finger print using Java Script as a plugin to Selenium.\n",
    "\n",
    "ex.) [Github curl_cffi](https://github.com/lexiforest/curl_cffi) | [Curl-cffi Documentation](https://curl-cffi.readthedocs.io/en/latest/impersonate.html)\n",
    "\n",
    "*This can work but, alone it is vulnerable to advanced bot detection such as CAPTCHAS.*\n",
    "\n",
    "+ It will mimic better the behavior of a browser vs the `request` library. Why is this different? Regular Python libraries emit data that will get you caught faster! This will allow you to impersonate various browsers and their correct credentials.\n",
    "\n",
    "\n",
    "\n",
    "`-------------------------------------------------------------`\n",
    "\n",
    "\n",
    "**IMPORTANT NOTE:** If you are using `open source software to avoid detection`, underatnd `CloudFlare` can also use this software to know how it works and fix vulnerabilities. Therfore, these technique may aid you for some services you encounter but, if you deal with advanced more guarded servers than you will be detected!\n",
    "\n",
    "`-------------------------------------------------------------`\n",
    "\n",
    "Here’s how you can handle these issues:\n",
    "\n",
    "+ `User Agent Rotation:` Changes the user agent to mimic different browsers and devices.\n",
    "+ `Delays:` Introduces random delays to avoid detection based on timing patterns.\n",
    "+ `Handling Cloudflare:` Cloudflare’s JavaScript challenges can be difficult to bypass with automation. Rotating IP addresses and using CAPTCHA-solving services might be necessary.\n",
    "\n",
    "`-------------------------------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0f4a3",
   "metadata": {},
   "source": [
    "# 7.) `Patterns & Target Area`\n",
    "\n",
    "**A.)** Here is an interesting topic: consider if you are trying to webscrape a company like `Amazon or Ebay` and you decide to enter a starting page that is very direct for your action but not something a typical user would use for a first start.\n",
    "\n",
    "Ex.) `amazon.com/[product_xyz]` this seems easy enough but this is not exactly a first match to what someone would type. It is unrealistic unless it is for something they viewed before or some other reason.\n",
    "\n",
    "Ex.) but the same person who didn't want to get caught as easily would do something more realistic `https://www.amazon.com/s?k=painters+tape&i=industrial&crid=36RXZ5Z6JCP2V&sprefix=pai%2Cindustrial%2C179&ref=nb_sb_ss_pltr-sample-20_1_3`\n",
    "\n",
    "or even: `https://www.amazon.com/s?k=painters+tape&i=industrial`\n",
    "\n",
    "I hope that even though this is generic, it does show that there is a difference because you are trying to look like an average user\n",
    "\n",
    "**B.)** Next, issue is this: if you were doing research or scraping in some European Super Market and your IP for some reason was in some place across the world it might trigger a response for weird behavior from the server to sniff you out.\n",
    "\n",
    "**C.)** Last, imagine if you are try to `randomize your scraping` this can help a lot. Scarpe different pages or different parts of a page over time.\n",
    "\n",
    "`-------------------------------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758faae",
   "metadata": {},
   "source": [
    "# 8.) `CAPTCHA`:\n",
    "\n",
    "+ If you need to solve captcha's then you need to consider a service such as `2captcha` or similar.\n",
    "\n",
    "+ For sites with less security you can try `undetected-chromedriver`\n",
    "\n",
    "+ \n",
    "\n",
    "`-------------------------------------------------------------`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c814cf",
   "metadata": {},
   "source": [
    "# `BONUS TIPS:`\n",
    "\n",
    "**`ALL Automation Tools have leaks, and create issues but here is a tip to help you out`**\n",
    "\n",
    "**1.)**\n",
    "\n",
    "`from selenium.webdriver import Chrome`\n",
    "\n",
    "`driver: Chrome`\n",
    "`script = \"Object.defineProperty(navigator, 'webdriver', {get: () => false})\"`\n",
    "`driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\"source\": script})`\n",
    "\n",
    "\n",
    "There is an interesting case here with this piece of code. A common leak that shows we are using a tool for automation/webscraping is the default function of `navigator.webdriver` this is a common issue in `Selenium, Puppeteer, Playwright`\n",
    "\n",
    "**2.)**\n",
    "\n",
    "At the time of launching your automation tools there are additional `flags in the background` that often create a `new leak` that you should close as well.\n",
    "\n",
    "You can take a log of the activity to see what is going on:\n",
    "\n",
    "`import logging`\n",
    "\n",
    "`selenium_logger = logging.getLogger('selenium.webdriver.remote.remote_connection')`\n",
    "\n",
    "`selenium_logger.setLevel(logging.DEBUG)`\n",
    "\n",
    "`logging.basicConfig(level=logging.DEBUG)`\n",
    "\n",
    "Then you can use something like this:\n",
    "\n",
    "`from selenium import webdriver\n",
    "\n",
    "`chromeOptions = webdriver.ChromeOptions()`\n",
    "\n",
    "`chromeOptions.add_experimental_option('excludeSwitches', ['disable-extensions','disable-default-apps','disable-component-extensions-with-background-pages'])`\n",
    "\n",
    "`chromeDriver = webdriver.Chrome(chrome_options=chromeOptions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Dom??\n",
    "\n",
    "#honey pots\n",
    "\n",
    "\n",
    "# https://medium.com/@pankaj_pandey/web-scraping-using-python-for-dynamic-web-pages-and-unveiling-hidden-insights-8dbc7da6dd26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15039c2e",
   "metadata": {},
   "source": [
    "# `Citations & Help:`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://www.zenrows.com/blog/selenium-avoid-bot-detection#how-anti-bots-work\n",
    "\n",
    "https://webscraping.ai/faq/headless-chromium/how-do-i-prevent-detection-of-headless-chromium-by-websites\n",
    "\n",
    "https://www.zenrows.com/blog/selenium-cloudflare-bypass#how-cloudflare-detect-selenium\n",
    "\n",
    "https://proxiesapi.com/articles/selenium-headless-stealth-tactics-to-bypass-cloudflare-detection\n",
    "\n",
    "https://webseekerj.medium.com/how-to-bypass-and-solve-cloudflare-captcha-with-python-selenium-32281fdf239e\n",
    "\n",
    "https://webscraping.ai/faq/http/can-i-use-http-2-for-web-scraping-and-what-are-the-benefits#:~:text=Yes%2C%20you%20can%20use%20HTTP,can%20enhance%20web%20scraping%20efficiency\n",
    "\n",
    "https://www.zenrows.com/blog/curl-cffi#step-2\n",
    "\n",
    "https://webscraping.fyi/lib/compare/python-curl-cffi-vs-python-undetected-chromedriver/\n",
    "\n",
    "https://www.zenrows.com/blog/curl-cffi#step-3\n",
    "\n",
    "https://www.zenrows.com/blog/pyppeteer#dynamic-pages\n",
    "\n",
    "https://scrapeops.io/web-scraping-playbook/web-scraping-without-getting-blocked/\n",
    "\n",
    "https://brightdata.com/blog/web-data/selenium-user-agent\n",
    "\n",
    "https://webseekerj.medium.com/change-the-user-agent-in-selenium-steps-best-practices-4e0f25438db3\n",
    "\n",
    "https://scrapfly.io/blog/how-to-avoid-web-scraping-blocking-javascript/\n",
    "\n",
    "https://medium.com/@yahyamrafe202/mastering-dynamic-web-scraping-from-challenges-to-solutions-with-playwright-088bfaa44a60\n",
    "\n",
    "https://www.zenrows.com/blog/selenium-headers#set-up-custom-headers\n",
    "\n",
    "https://pypi.org/project/selenium-wire/\n",
    "\n",
    "https://scrapeops.io/selenium-web-scraping-playbook/python-selenium-wire/#modifying-request-headers\n",
    "\n",
    "https://medium.com/@dungwoong/pretending-im-a-human-while-web-scraping-d5464e36f24\n",
    "\n",
    "https://datawookie.dev/blog/2023/03/chrome-devtools-protocol-selenium/\n",
    "\n",
    "https://www.scrapingbee.com/blog/selenium-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
