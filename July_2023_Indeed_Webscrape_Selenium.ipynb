{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a55bed64",
   "metadata": {},
   "source": [
    "# `Selenium Webscraping Indeed Job Postings - July 2023`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "# `Purpose & Outcome:`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf01d1",
   "metadata": {},
   "source": [
    "# `What is Selenium and how is it used?`\n",
    "\n",
    "+ When you need to do unit testing, automation or assistance when webscraping this is a tool to aid you.\n",
    "    + Great for clicking buttons\n",
    "    + drop-down menus\n",
    "    + acting/emulating human interactions on a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "653f45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if you have never used these: unblock the lines below to install if needed\n",
    "\n",
    "# !pip install webdriver-manager\n",
    "# !pip3 install lxml\n",
    "# !pip3 install selenium\n",
    "# !pip3 install webdriver_manager\n",
    "# !pip install --upgrade pip\n",
    "# !pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d754db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- import necessary modules -------\n",
    "\n",
    "# For webscraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing and creating xml data\n",
    "from lxml import etree as et\n",
    "\n",
    "# Store data as a csv file written out\n",
    "from csv import writer\n",
    "\n",
    "# In general to use with timing our function calls to Indeed\n",
    "import time\n",
    "\n",
    "# Assist with creating incremental timing for our scraping to seem more human\n",
    "from time import sleep\n",
    "\n",
    "# Dataframe stuff\n",
    "import pandas as pd\n",
    "\n",
    "# Random integer for more realistic timing for clicks, buttons and searches during scraping\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bee43e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.10.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import selenium\n",
    "\n",
    "# Check version I am running\n",
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba65a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium 4:\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f41e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows searchs similar to beautiful soup: find_all\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Try to establish wait times for the page to load\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Wait for specific condition based on defined task: web elements, boolean are examples\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Used for keyboard movements, up/down, left/right,delete, etc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Locate elements on page and throw error if they do not exist\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c820c",
   "metadata": {},
   "source": [
    "# `Consider Headless Browser: speed up & use less resources:`\n",
    "\n",
    "There are some condiserations though:\n",
    "\n",
    "+ Some browsers create issues\n",
    "+ debugging can be tricky\n",
    "+ you may have limited plugin usage or support\n",
    "+ you are not able to see visually how the website or application are working \n",
    "\n",
    "# `from selenium.webdriver.common.by import By`\n",
    "\n",
    "Think of this as being similar to using `Beautiful Soup and find_all`\n",
    "+ when used it allows you to find something within an HTML document, if it fails you raise the exception: `NoSuchElementException`\n",
    "+ **`Becareful when using BY`** because if this is not a static page then any attrubutes you are searching can become an error in the future when it fails.\n",
    "    + For example if you are searching by `Class` this can create issues later vs using\n",
    "        + This is because it is a `CSS` selector and can change overtime since it is an attribute\n",
    "    + `ID` which may make your code more robust! This CAN be a unique identifier that may help you instead\n",
    "\n",
    "# `NoSuchElementException`\n",
    "\n",
    "This is useful to locate elements within a page while loading and try to handle exceptions.\n",
    "+ During `AJAX` calls you may have issues if the application was build using `React, VUE, Angular` and require different use cases to make the above checks. [article to explain](https://reflect.run/articles/everything-you-need-to-know-about-nosuchelementexception-in-selenium/) and you can consider polling.\n",
    "\n",
    "`--------------------------------`\n",
    "\n",
    "# `Other Common Errors:`\n",
    "\n",
    "+ **`InvalidSelectorException`**\n",
    "\n",
    "+ **`ElementNotInteractableException`**\n",
    "\n",
    "+ **`TimeoutException`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6c62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "option= webdriver.ChromeOptions()\n",
    "\n",
    "# Going undercover:\n",
    "option.add_argument(\"--incognito\")\n",
    "\n",
    "\n",
    "# # Consider this if the application works and you know how it works for speed ups and rendering!\n",
    "\n",
    "# option.add_argument('--headless=chrome')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "ae3a9129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/jobs?q={}&l={}&radius=35&filter=0&sort=date&start={}\n"
     ]
    }
   ],
   "source": [
    "# Define job and location search keywords\n",
    "job_search_keyword = ['Data+Scientist', 'Business+Analyst', 'Data+Engineer', \n",
    "                      'Python+Developer', 'Full+Stack+Developer', 'Machine+Learning+Engineer']\n",
    "\n",
    "# Define Locations of Interest\n",
    "location_search_keyword = ['New+York', 'California', 'Washington']\n",
    "\n",
    "# Define base and pagination URL's\n",
    "base_url = 'https://www.indeed.com'\n",
    "\n",
    "# Finding location, position, radius=35 miles, sort by date and starting page\n",
    "paginaton_url = 'https://www.indeed.com/jobs?q={}&l={}&radius=35&filter=0&sort=date&start={}'\n",
    "\n",
    "print(paginaton_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30276a3f",
   "metadata": {},
   "source": [
    "# Things to consider:\n",
    "\n",
    "+ Wait for page to load before we start running tasks\n",
    "+ make sure what we are looking for is actually there\n",
    "    + It can be absent\n",
    "    + hidden in DOM, iframe or similar\n",
    "+ timing our calls to remain more like an average user\n",
    "+ Exception handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447966c",
   "metadata": {},
   "source": [
    "# `Let's Look at what is going on below:`\n",
    "\n",
    "There are some concerns and things for you to consider:\n",
    "\n",
    "1.) Below is the MAX number of jobs you will find for a posting of interest!\n",
    "\n",
    "2.) This is not an accurate depiction because you can have way less than this depending on results\n",
    "    \n",
    "    + An issue arises due to duplicated listings\n",
    "\n",
    "3.) Pagination is difficult to do and when to stop the search results\n",
    "\n",
    "4.) you have a filter option (&filter=0, &filter=1), filter =1 shows non-duplicates which reduces results but you need to figure out how to do pagination!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "3bed8c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_='Data+Engineer'\n",
    "location='Washington'\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "\n",
    "driver.get(paginaton_url.format(job_,location,0))\n",
    "sleep(randint(2, 6))\n",
    "p=driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "\n",
    "u=math.ceil(int(p.split(' ')[0])/15)\n",
    "driver.quit()\n",
    "u\n",
    "\n",
    "# for i in range(0,11*10+10,10):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "d0a53efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagination: PRACTICE\n",
    "\n",
    "job_='Data+Engineer'\n",
    "location='Washington'\n",
    "# page=[0,10,20]\n",
    "\n",
    "\n",
    "job_lst=[]\n",
    "\n",
    "\n",
    "job_description_list = []\n",
    "salary_list=[]\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "# driver.get(\"https://www.indeed.com/q-USA-jobs.html\")\n",
    "\n",
    "# for i in page:\n",
    "for i in range(0,u):\n",
    "    driver.get(paginaton_url.format(job_,location,i*10))\n",
    "    sleep(randint(2, 4))\n",
    "\n",
    "    job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "    jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "\n",
    "    for jj in jobs:\n",
    "        job_title = jj.find_element(By.CLASS_NAME,\"jobTitle\")\n",
    "#         print(job_title.text)\n",
    "        \n",
    "# Href's to get full job description (need to re-terate to get full info)\n",
    "# Reference ID for each job used by indeed         \n",
    "# Finding the company name        \n",
    "# Location\n",
    "# Posting date\n",
    "# Job description\n",
    "\n",
    "        job_lst.append([job_title.text,\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\"),\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"id\"),      \n",
    "        jj.find_element(By.CLASS_NAME,\"companyName\").text,       \n",
    "        jj.find_element(By.CLASS_NAME,\"companyLocation\").text,\n",
    "        jj.find_element(By.CLASS_NAME,\"date\").text,\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\")])\n",
    "    \n",
    "    \n",
    "        # Job Description Links for further parsing and scraping text\n",
    "#         job_description_list.append(job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\"))\n",
    "        \n",
    "\n",
    "        try: \n",
    "            salary_list.append(jj.find_element(By.CLASS_NAME,\"metadata salary-snippet-container\"))\n",
    "\n",
    "        except NoSuchElementException: \n",
    "            try: \n",
    "                salary_list.append(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                salary_list.append(None)\n",
    "\n",
    "                \n",
    "                \n",
    "        # Click the job element to get the description\n",
    "        job_title.click()\n",
    "        \n",
    "        # Help to load page so we can find and extract data\n",
    "        sleep(randint(3, 5))\n",
    "\n",
    "        try: \n",
    "            job_description_list.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    "            \n",
    "        except: \n",
    "            \n",
    "            job_description_list.append(None)\n",
    "\n",
    "            \n",
    "# alternate way to grab the info for job description to make it faster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c39477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get links and take description:\n",
    "\n",
    "# Salary if available and use exceptions\n",
    "\n",
    "# remote or not! parse with regex and use a separate column for df\n",
    "\n",
    "#setup for number of jobs and parse by that as iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "4bdfa078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Software Engineer, Big Data Processing',\n",
       " 'https://www.indeed.com/rc/clk?jk=9a67e5a624e1f04d&fccid=642127b94898142b&vjs=3',\n",
       " 'job_9a67e5a624e1f04d',\n",
       " 'SpaceCurve',\n",
       " 'Seattle, WA 98104',\n",
       " 'Posted\\nPosted 30+ days ago',\n",
       " 'https://www.indeed.com/rc/clk?jk=9a67e5a624e1f04d&fccid=642127b94898142b&vjs=3']"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dates using date time\n",
    "# job_lst[-2]\n",
    "\n",
    "# job_description_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "4e0f185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Engineer\n",
      "None\n",
      "Artica - Senior Data Applied Science Engineer (Seattle, WA) - Direct Hire [Hybrid]\n",
      "$180,000 - $200,000 a year\n",
      "Senior Data Engineer (US Remote)\n",
      "Estimated $125K - $158K a year\n",
      "Data Engineer\n",
      "$140,000 - $190,000 a year\n",
      "Staff Software Engineer - Data Science\n",
      "$149,240 - $200,200 a year\n",
      "Staff Software Engineer - Data Integration\n",
      "$149,240 - $200,200 a year\n",
      "Principal Software Engineer (Data), Industry Solutions Engineering\n",
      "$133,600 - $256,800 a year\n",
      "Data Engineer, AET Central Services Technology - Data\n",
      "From $105,700 a year\n",
      "Software Engineer, Data Platform\n",
      "Estimated $137K - $174K a year\n",
      "Customer Engineer, Data Analytics, Google Cloud\n",
      "None\n",
      "Software Engineer, Data Platform\n",
      "None\n",
      "Senior Backend Engineer - Data\n",
      "$140,000 - $215,000 a year\n",
      "Staff Full Stack Engineer - Data\n",
      "$175,000 - $225,000 a year\n",
      "Distributed Data Systems - Staff Software Engineer\n",
      "$182,400 - $247,000 a year\n",
      "Data Center Operations Engineer (Kirkland, WA)\n",
      "$45 - $51 an hour\n",
      "Senior Software Engineer - Distributed Data Systems\n",
      "$157,700 - $213,800 a year\n",
      "Lead Software Engineer; Cloud and Big Data\n",
      "$137,750 - $200,000 a year\n",
      "Senior Data Engineer, Kuiper Business Development\n",
      "From $123,700 a year\n",
      "Principal Software Engineer, Data (Starshield)\n",
      "$200,000 - $270,000 a year\n",
      "mgr engineer - Data Engineering\n",
      "None\n",
      "Data Engineer, Data Platform - US Tech Services Team\n",
      "$177,688 - $341,734 a year\n",
      "Senior Data Engineer\n",
      "Estimated $133K - $168K a year\n",
      "Senior Data Engineer\n",
      "$187,000 - $190,300 a year\n",
      "Senior Data Engineer\n",
      "$125,000 - $155,000 a year\n",
      "Site Reliability Engineer, Data Platform-TikTok-US-Tech Services\n",
      "$129,960 - $246,240 a year\n",
      "Solutions Engineer - Data & Machine Learning\n",
      "$115,000 - $150,000 a year\n",
      "Senior Software Engineer, TikTok Protected Data Infrastructure\n",
      "$175,750 - $266,000 a year\n",
      "Senior Data Engineer - Data Warehouse - Hybrid In Seattle WA or Frisco TX\n",
      "$115,300 - $172,800 a year\n",
      "Sr. Data Engineer\n",
      "$100,531 - $138,230 a year\n",
      "Tech Lead Software Engineer, TikTok Protected Data Infrastructure\n",
      "$199,500 - $340,100 a year\n",
      "SLD Senior Avionics Command and Data Handling Responsible Engineer - Lunar Transportation\n",
      "$125,400 - $183,920 a year\n",
      "Data Engineer\n",
      "Estimated $113K - $143K a year\n",
      "Data Engineer (Starlink)\n",
      "$120,000 - $145,000 a year\n",
      "Data Acquisition Software Engineer\n",
      "$112,000 - $215,000 a year\n",
      "Senior Business Intelligence Engineer - Data Center Learning, Data Center Learning\n",
      "From $104,300 a year\n",
      "Senior Data Engineer\n",
      "Estimated $137K - $173K a year\n",
      "Senior Cloud Data Engineer - Richland, WA\n",
      "$132,900 a year\n",
      "Data Engineer II or III\n",
      "$125,612 a year\n",
      "Data Engineer, Ops\n",
      "$95,000 - $130,000 a year\n",
      "Data Engineer\n",
      "None\n",
      "Data Engineer, Product Analytics\n",
      "$165,000 - $232,000 a year\n",
      "Staff Data Engineer\n",
      "$143,400 - $175,300 a year\n",
      "Senior Data Engineer\n",
      "None\n",
      "Sr. Data and Storage Engineer\n",
      "$97,993 - $146,989 a year\n",
      "Senior Software Engineer, Data Platform - Remote\n",
      "$142,000 - $213,000 a year\n"
     ]
    }
   ],
   "source": [
    "paginaton_url_ = 'https://www.indeed.com/jobs?q={}&l={}&sort=date&start={}'\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "p_=[]\n",
    "salary_list_=[]\n",
    "for i in range(0,3):\n",
    "    driver.get(paginaton_url_.format(job_,location,i*10))\n",
    "    sleep(randint(2, 3))\n",
    "    \n",
    "    job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "    jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "    \n",
    "    for jj in jobs:\n",
    "        job_title = jj.find_element(By.CLASS_NAME,\"jobTitle\")\n",
    "        print(job_title.text)\n",
    "        p_.append(job_title.text)\n",
    "#         sleep(randint(3, 5))\n",
    "        try:\n",
    "            salary_list_.append(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "            print(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "#         sleep(randint(1, 2))\n",
    "#         try: \n",
    "#             salary_list_.append(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "#             print(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "        except: \n",
    "            try: \n",
    "                salary_list.append(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "                print(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "            except:\n",
    "                print('None')\n",
    "                \n",
    "driver.quit()\n",
    "\n",
    "# //*[@id=\"challenge-stage\"]/div/label/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "ad8b352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6730c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df and store data\n",
    "\n",
    "\n",
    "# duplicate entries remove!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "80c2e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf1ed3",
   "metadata": {},
   "source": [
    "# `Here is a side note:`\n",
    "\n",
    "\n",
    "+ This gives me an error because it was code from the past version:\n",
    "\n",
    "`driver = webdriver.Chrome(ChromeDriverManager().install())`\n",
    "\n",
    "\n",
    "+ `When using ingonito browser:` your browsing tabs will pull different data than a normal window. Understand this when doing your troubleshooting and debugging. If you have a window open to find your tags but parse in a different type of window the results will not line up.\n",
    "\n",
    "+ Also, when you are grabbing `job descriptions` for example you will need to time it so the page will read the data after it is loaded. If you immediately try to grab data you may not get everything!\n",
    "    + Option 1: use the clickable tab from the `job title` then scrape directly\n",
    "    + Option 2: consider saving the `HREF's` and then doing a separate parsing in a different function. This I think may be faster. But, check for yourself.\n",
    "    \n",
    "+ To speed things up consider `headless browser` but, understand the debugging becomes an issue!\n",
    "\n",
    "+ **If you parse a good amount of pages** you will encounter a checkbox that needs to be clicked to show you are not a robot. This occurs to me usually after 15-30 pages of scraping which is not a lot. (I need to figure this out)\n",
    "    + Option 1: try to see if you can pull the information for this button to scrape it directly and click\n",
    "    + Option 2: reset and tinker with the settings of timing out, sleep settings and maybe error handling\n",
    "\n",
    "**`Big Concern: Pagination`**\n",
    "When you need to go from page to page sequentially this is not straight forward. Practice and a lot of reading will aid you. I am not savvy just yet.\n",
    "+ Clickable buttons and learning how to use them and WHEN TO STOP iterating are NOT trivial tasks\n",
    "+ Hacking your way through, such I did for this example but, there is a glaring issue with duplicate entries.\n",
    "+ Finding hidden elements and figuring out how to extract them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4260603c",
   "metadata": {},
   "source": [
    "# First let's try to find the number of jobs for a given posting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ee10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07946ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36dd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.find_element(By.ID,\"mosaic-jobResults\").text\n",
    "# (\"//div[@class='jobsearch-SerpJobCard unifiedRow row result clickcard']\")\n",
    "# .text\n",
    "# find_element_by_xpath(\"//div[@class='jobsearch-SerpJobCard unifiedRow row result clickcard']\").text\n",
    "\n",
    "\n",
    "# 1.) iterate over jobs, locations\n",
    "# 2.) find the elements from outer to inner\n",
    "# 3.) extract everything then go to next page\n",
    "# 4.) medium article for each pagination\n",
    "# 5.) error handling & timing, clicking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a few options:\n",
    "\n",
    "# 1.) Try to use incognito -----------(done)\n",
    "# 2.) Maybe I should use random int for sleep ----(good idea)\n",
    "# 3.) What to do when I have the human click button for pop up\n",
    "# 4.) verify if the \"+\" symbols are needed look at formatting ----------(done)\n",
    "# 5.) check if the formatting to parse is the same or not for div tags, etc\n",
    "# 6.) do I need to use headless browser? (Possible issues! investigate)\n",
    "# 7.) locating elements using the BY package, similar to beautiful soup find_all\n",
    "# 8.) errors with NoSuchElementException ----------------(done)\n",
    "# 9.) try to identify code that doesn't change over time\n",
    "# 10.) Xpath to find buttons to go page by page and contain arrows forward/backward with try.except"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9aeda",
   "metadata": {},
   "source": [
    "# Notes for this project:\n",
    "\n",
    "+ Filling in forms:\n",
    "+ click buttons\n",
    "+ possible human detection stuff\n",
    "\n",
    "**`Xpath vs CSS selectors for retreiving data`**\n",
    "\n",
    "+ `Xpath:` bidirectional (can go from parent to child and reverse) traversal\n",
    "    + slower retrevial speed\n",
    "    + text functions supported\n",
    "    + pay attention to relative '//' and absolute path '/' notations\n",
    "    + Think of a tree like structure to breakdown\n",
    "+ `CSS:` directional (parent to child only)\n",
    "\n",
    "`------------------------`\n",
    "\n",
    "**`Xpath`**\n",
    "+ *`Xpath`* stands for `XML Path` which is a query language used to find the path of an element in XML documents\n",
    "+ Essentially you are navigating a `DOM` \n",
    "+ More flexible than using `CSS`\n",
    "    + If you don't know the name of an element you can use `contains` as your key word which is great!\n",
    " \n",
    "**`CSS`**\n",
    "+ Most often the HTML will be styled in a cascading format and identifying elements will come from the `Class` they fall within\n",
    "+ They are used to select various elements within a `DOM`\n",
    "    + **`Simple selectors:`** such as finding a `Class` or `ID`\n",
    "    + **`Attribute selectors:`** \n",
    "    + **`Pseudo selectors:`** such as hover boxes or check boxes as examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9cef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.indeed.com/q-USA-jobs.html?vjk=823cd7ee3c203ac3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86595dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get DOM from given URL\n",
    "def get_dom(url):\n",
    "    driver.get(url)\n",
    "    page_content = driver.page_source\n",
    "    product_soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    dom = et.HTML(str(product_soup))\n",
    "    return dom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in get_dom(url):\n",
    "#     print(et.tostring(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to extract job link\n",
    "# def get_job_link(job):\n",
    "#     practice_=[]\n",
    "#     try:\n",
    "#         job_link = job.xpath('./descendant::h2/a/@href')[0]\n",
    "#     except Exception as e:\n",
    "#         job_link = 'Not available'\n",
    "#     practice_.append(job_link)\n",
    "# #     return job_link\n",
    "#     return practice_\n",
    "\n",
    "# for job_keyword in job_search_keyword:\n",
    "#     for location_keyword in location_search_keyword:\n",
    "#         all_jobs = []\n",
    "#         for page_no in range(0, 10, 10):\n",
    "#             url = paginaton_url.format(job_keyword, location_keyword, page_no)\n",
    "#             page_dom = get_dom(url)\n",
    "#             jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
    "#         all_jobs.append(jobs)\n",
    "# #             all_jobs = all_jobs + jobs\n",
    "# #     print(all_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746df1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9a1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a CSV file to write the job listings data\n",
    "# with open('indeed_jobs1.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "#     theWriter = writer(f)\n",
    "#     heading = ['job_link', 'job_title', 'company_name', 'company_location', 'salary', 'job_type', 'rating', 'job_description', 'searched_job', 'searched_location']\n",
    "#     theWriter.writerow(heading)\n",
    "#     for job_keyword in job_search_keyword:\n",
    "#         for location_keyword in location_search_keyword:\n",
    "#             all_jobs = []\n",
    "#             for page_no in range(0, 100, 10):\n",
    "#                 url = paginaton_url.format(job_keyword, location_keyword, page_no)\n",
    "#                 page_dom = get_dom(url)\n",
    "#                 jobs = page_dom.xpath('//div[@class=\"job_seen_beacon\"]')\n",
    "# #                 all_jobs = all_jobs +jobs\n",
    "#                 print(all_jobs+jobs)\n",
    "#                 all_jobs_ = all_jobs.append(jobs) #changed here and below\n",
    "#                 print(\"yay\",all_jobs_)\n",
    "#             for job in all_jobs_:\n",
    "#                 job_link = base_url + get_job_link(job)\n",
    "#                 time.sleep(2)\n",
    "#                 job_title = get_job_title(job)\n",
    "#                 time.sleep(2)\n",
    "#                 company_name = get_company_name(job)\n",
    "#                 time.sleep(2)\n",
    "#                 company_location = get_company_location(job)\n",
    "#                 time.sleep(2)\n",
    "#                 salary = get_salary(job)\n",
    "#                 time.sleep(2)\n",
    "#                 job_type = get_job_type(job)\n",
    "#                 time.sleep(2)\n",
    "#                 rating = get_rating(job)\n",
    "#                 time.sleep(2)\n",
    "#                 job_desc = get_job_desc(job)\n",
    "#                 time.sleep(2)\n",
    "#                 record = [job_link, job_title, company_name, company_location, salary, job_type, rating, job_desc, job_keyword, location_keyword]\n",
    "#                 theWriter.writerow(record)\n",
    "\n",
    "# # Closing the web browser\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa12736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ff3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pd.read_csv('indeed_jobs1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e00647",
   "metadata": {},
   "source": [
    "# Like, Share & <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebeef3e",
   "metadata": {},
   "source": [
    "# `Citations & Help:`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://pypi.org/project/webdriver-manager/\n",
    "\n",
    "https://www.blog.datahut.co/post/scrape-indeed-using-selenium-and-beautifulsoup\n",
    "\n",
    "https://github.com/henrionantony/Dynamic-Web-Scraping-using-Python-and-Selenium/blob/master/indeed.py\n",
    "\n",
    "https://www.specrom.com/blog/web-scraping-job-postings-on-indeed-using-python/\n",
    "\n",
    "https://www.scrapingdog.com/blog/scrape-indeed-using-python/ (bs4 as of Feb 13, 2023)\n",
    "\n",
    "https://selenium-python.readthedocs.io/locating-elements.html#locating-elements\n",
    "\n",
    "https://stackoverflow.com/questions/50865088/how-to-get-string-dump-of-lxml-element\n",
    "\n",
    "https://selenium-python.readthedocs.io/navigating.html\n",
    "\n",
    "https://towardsdatascience.com/web-scraping-job-postings-from-indeed-com-using-selenium-5ae58d155daf (2020 version)\n",
    "\n",
    "https://www.pycodemates.com/2022/01/Indeed-jobs-scraping-with-python-bs4-selenium-and-pandas.html\n",
    "\n",
    "https://medium.com/forcodesake/how-to-build-a-scraping-tool-for-indeed-in-8-minutes-data-science-csv-selenium-beautifulsoup-python-95fcca4b9719 (Good Read & Adapted Code)\n",
    "\n",
    "https://www.tutorialspoint.com/how-to-open-browser-window-in-incognito-private-mode-using-python-selenium-webdriver\n",
    "\n",
    "https://www.selenium.dev/selenium/docs/api/py/webdriver/selenium.webdriver.common.keys.html\n",
    "\n",
    "https://pythonbasics.org/selenium-wait-for-page-to-load/\n",
    "\n",
    "https://www.seleniumeasy.com/selenium-tutorials/selenium-headless-browser-execution\n",
    "\n",
    "https://www.browserstack.com/guide/expectedconditions-in-selenium\n",
    "\n",
    "https://www.testim.io/blog/xpath-vs-css-selector-difference-choose/\n",
    "\n",
    "https://www.w3.org/TR/REC-DOM-Level-1/introduction.html\n",
    "\n",
    "https://github.com/diego-florez/Selenium-Web-Scraping/blob/master/indeed.py (Indeed scrape Selenium 2020) error Handling also\n",
    "\n",
    "https://www.testim.io/blog/selenium-click-button/\n",
    "\n",
    "https://scrapfly.io/blog/how-to-scrape-indeedcom/\n",
    "\n",
    "https://goh.physics.ucdavis.edu/datascience/webscraping/webscraping.html\n",
    "\n",
    "https://levelup.gitconnected.com/efficiently-scraping-multiple-pages-of-data-a-guide-to-handling-pagination-with-selenium-and-3ed93857f596\n",
    "\n",
    "https://github.com/israel-dryer/Indeed-Job-Scraper/blob/master/indeed-job-scraper-selenium.ipynb\n",
    "\n",
    "https://www.zenrows.com/blog/headless-browser-python#switch-to-python-selenium-headless-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90a0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
