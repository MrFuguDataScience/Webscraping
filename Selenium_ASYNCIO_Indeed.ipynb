{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51e24ac",
   "metadata": {},
   "source": [
    "# `Selenium Indeed Webscrape Speed Up: ASYNIO July 2023`\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "# `Outcome & Purpose:`\n",
    "\n",
    "+ Webscrape Indeed\n",
    "+ Speed up slow processes\n",
    "+ convert to DF and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa85be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if you have never used these: unblock the lines below to install if needed\n",
    "\n",
    "# !pip install webdriver-manager\n",
    "# !pip3 install lxml\n",
    "# !pip3 install selenium\n",
    "# !pip3 install webdriver_manager\n",
    "# !pip install --upgrade pip\n",
    "# !pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b158e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing and creating xml data\n",
    "from lxml import etree as et\n",
    "\n",
    "# Store data as a csv file written out\n",
    "from csv import writer\n",
    "\n",
    "# In general to use with timing our function calls to Indeed\n",
    "import time\n",
    "\n",
    "# Assist with creating incremental timing for our scraping to seem more human\n",
    "from time import sleep\n",
    "\n",
    "# Dataframe stuff\n",
    "import pandas as pd\n",
    "\n",
    "# Random integer for more realistic timing for clicks, buttons and searches during scraping\n",
    "from random import randint\n",
    "\n",
    "# Multi Threading\n",
    "import threading\n",
    "\n",
    "# Threading:\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "# Check version I am running\n",
    "selenium.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium 4:\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# Starting/Stopping Driver: can specify ports or location but not remote access\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "# Manages Binaries needed for WebDriver without installing anything directly\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820706d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows searchs similar to beautiful soup: find_all\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Try to establish wait times for the page to load\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# Wait for specific condition based on defined task: web elements, boolean are examples\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Used for keyboard movements, up/down, left/right,delete, etc\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "# Locate elements on page and throw error if they do not exist\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb822fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows you to cusotmize: ingonito mode, maximize window size, headless browser, disable certain features, etc\n",
    "option= webdriver.ChromeOptions()\n",
    "\n",
    "# Going undercover:\n",
    "option.add_argument(\"--incognito\")\n",
    "option.add_argument(\"--headless=new\")\n",
    "\n",
    "# Finding location, position, radius=35 miles, sort by date and starting page\n",
    "paginaton_url = 'https://www.indeed.com/jobs?q={}&l={}&radius=35&filter=0&sort=date&start={}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae788d9b",
   "metadata": {},
   "source": [
    "# `Asynchronous Code:`\n",
    "\n",
    "**What is really going on here?**\n",
    "\n",
    "Essentially, you are splitting routines which may become busy and move to a next step or procedure until the workload clears up. This is allowing a `concurrent` use of time and space. \n",
    "\n",
    "+ Think of doing individual actions one-by-one and having to wait for the next task (SYNCRONOUS) workload\n",
    "    + We get a speed up but not exactly Multi-core processing. It is a little different. We are freeing up resources to work on other tasks while one or more are busy and then come back to them later.\n",
    "\n",
    "\n",
    "`-----------------------------------------`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9566a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp) (3.4)\n",
      "Collecting asyncio\n",
      "  Downloading asyncio-3.4.3-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: asyncio\n",
      "Successfully installed asyncio-3.4.3\n"
     ]
    }
   ],
   "source": [
    "# For Our Speed Up: AsyncIO and AIOHttp\n",
    "\n",
    "!pip install aiohttp\n",
    "!pip install asyncio\n",
    "\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "job_='Data+Engineer'\n",
    "location='Washington'\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "\n",
    "\n",
    "driver.get(paginaton_url.format(job_,location,0))\n",
    "\n",
    "# t = ScrapeThread(url_)\n",
    "# t.start()\n",
    "\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "p=driver.find_element(By.CLASS_NAME,'jobsearch-JobCountAndSortPane-jobCount').text\n",
    "\n",
    "# Max number of pages for this search! There is a caveat described soon\n",
    "max_iter_pgs=int(p.split(' ')[0])//15 \n",
    "\n",
    "\n",
    "driver.quit() # Closing the browser we opened\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start,'seconds to complete action!')\n",
    "print('-----------------------')\n",
    "print('Max Iterable Pages for this search:',max_iter_pgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7423a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagination: PRACTICE\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "job_='Data+Engineer'\n",
    "location='Washington'\n",
    "\n",
    "\n",
    "job_lst=[]\n",
    "job_description_list_href=[]\n",
    "\n",
    "# job_description_list = []\n",
    "salary_list=[]\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),\n",
    "                         options=option)\n",
    "sleep(randint(2, 6))\n",
    "\n",
    "\n",
    "for i in range(0,max_iter_pgs):\n",
    "    driver.get(paginaton_url.format(job_,location,i*10))\n",
    "    \n",
    "    \n",
    "    sleep(randint(2, 4))\n",
    "\n",
    "    job_page = driver.find_element(By.ID,\"mosaic-jobResults\")\n",
    "    jobs = job_page.find_elements(By.CLASS_NAME,\"job_seen_beacon\") # return a list\n",
    "\n",
    "    for jj in jobs:\n",
    "        job_title = jj.find_element(By.CLASS_NAME,\"jobTitle\")\n",
    "        \n",
    "# Href's to get full job description (need to re-terate to get full info)\n",
    "# Reference ID for each job used by indeed         \n",
    "# Finding the company name        \n",
    "# Location\n",
    "# Posting date\n",
    "# Job description\n",
    "\n",
    "        job_lst.append([job_title.text,\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\"),\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"id\"),      \n",
    "        jj.find_element(By.CLASS_NAME,\"companyName\").text,       \n",
    "        jj.find_element(By.CLASS_NAME,\"companyLocation\").text,\n",
    "        jj.find_element(By.CLASS_NAME,\"date\").text,\n",
    "        job_title.find_element(By.CSS_SELECTOR,\"a\").get_attribute(\"href\")])\n",
    "        \n",
    "\n",
    "        try: # I removed the metadata attached to this class name to work!\n",
    "            salary_list.append(jj.find_element(By.CLASS_NAME,\"salary-snippet-container\").text)\n",
    "\n",
    "        except NoSuchElementException: \n",
    "            try: \n",
    "                salary_list.append(jj.find_element(By.CLASS_NAME,\"estimated-salary\").text)\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                salary_list.append(None)\n",
    "      \n",
    "                \n",
    "#         # Click the job element to get the description\n",
    "#         job_title.click()\n",
    "        \n",
    "#         # Help to load page so we can find and extract data\n",
    "#         sleep(randint(3, 5))\n",
    "\n",
    "#         try: \n",
    "#             job_description_list.append(driver.find_element(By.ID,\"jobDescriptionText\").text)\n",
    "            \n",
    "#         except: \n",
    "            \n",
    "#             job_description_list.append(None)\n",
    "\n",
    "driver.quit() \n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start,'seconds to complete Query!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64431ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fea38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8210be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cd638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dee3356",
   "metadata": {},
   "source": [
    "# Like, Share & <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70256639",
   "metadata": {},
   "source": [
    "# `Citations & Help:`\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "\n",
    "Code Optimizing with Asynio, multi-threading and multi-processing:\n",
    "\n",
    "https://www.geeksforgeeks.org/multithreading-or-multiprocessing-with-python-and-selenium/\n",
    "\n",
    "https://www.youtube.com/watch?v=-hw3AaxX5B4\n",
    "\n",
    "https://webnus.net/how-to-speed-up-selenium-automated-tests-in-2022/ (selenium speed up ideas)\n",
    "\n",
    "https://medium.com/@PhysicistMarianna/scrape-job-postings-data-from-indeed-com-with-python-b4f31340ef5f (bs4 help maybe)\n",
    "\n",
    "https://github.com/Ram-95/Indeed_Job_Scraper/blob/master/Indeed_Job_Scraper.py (bs4 idea as well)\n",
    "\n",
    "https://www.youtube.com/watch?v=HOS5Hix--bE\n",
    "\n",
    "https://stackoverflow.com/questions/75849391/failed-to-fetch-the-job-titles-from-indeed-using-the-requests-module (cloudscraper idea)\n",
    "\n",
    "https://www.geeksforgeeks.org/multithreading-python-set-1/ (multi-threading ex.)\n",
    "\n",
    "https://testdriven.io/blog/building-a-concurrent-web-scraper-with-python-and-selenium/ (come back to this! good write up with code...)\n",
    "\n",
    "https://medium.com/analytics-vidhya/asynchronous-web-scraping-101-fetching-multiple-urls-using-arsenic-ec2c2404ecb4\n",
    "\n",
    "https://www.youtube.com/watch?v=6ow7xloFy5s\n",
    "\n",
    "https://us-pycon-2019-tutorial.readthedocs.io/aiohttp_intro.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
